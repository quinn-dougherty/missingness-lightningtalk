{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doing binning\n",
      "doing dates\n",
      "doing insig drop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from functools import reduce\n",
    "\n",
    "from pandas import DataFrame as DF\n",
    "#from altair import Chart # for type aliasing\n",
    "\n",
    "gh_raw_prefix = 'https://raw.githubusercontent.com/quinn-dougherty/well/master/'\n",
    "\n",
    "csv_local = ['train_features.csv', 'test_features.csv', 'train_labels.csv', 'sample_submission.csv']\n",
    "csv_github = {x: gh_raw_prefix + x for x in csv_local}\n",
    "\n",
    "df = pd.read_csv(csv_github[csv_local[0]]).sample(8000)\n",
    "y = pd.read_csv(csv_github[csv_local[2]])\n",
    "\n",
    "to_drop = ['scheme_name', 'wpt_name', 'name']\n",
    "\n",
    "null_vals_list = ['Not Known', 'Unknown', 'None', 'Not known', 'not known',\n",
    "                  '-', 'unknown', 'Unknown Installer', '##', 'none', '0']\n",
    "\n",
    "boolski = ['public_meeting', 'permit']\n",
    "\n",
    "to_bin = {'construction_year': [-1, 1980, 1990, 2000, 2010, 2020], \n",
    "          'population': [-1, 10, 20, 100, 250, 1000, 5000, 10000, 100000]}\n",
    "\n",
    "insigs = ['funder', 'installer', 'subvillage', 'ward']\n",
    "\n",
    "dates = ['date_recorded']\n",
    "\n",
    "twoprodc = ['region', 'district_code']\n",
    "t = (twoprodc[0], twoprodc[1])\n",
    "\n",
    "def const_impute(datf: DF, nl: List[str], const: str = \"NOT_KNOWN\") -> DF: \n",
    "  '''This is a naive and destructive fillna impute. \n",
    "  Given a dataframe and a list of strings known to be synonymous with \"null\"\n",
    "  mutate the dataframe by replacing both missing vals and occurrences of vals in the given list \n",
    "  with a constant such as \"NOT_KNOWN\"\n",
    "  \n",
    "  also return the dataframe. '''\n",
    "  return datf.replace({x: np.nan for x in nl}).fillna(const)\n",
    "\n",
    "def bool_to_numeric(datf: DF, feats: List[str], unknown: float = 0.5) -> DF:\n",
    "  \"\"\"Bool to numeric\"\"\"\n",
    "  mapped = datf[feats].replace({True: 1, False: 0, 'NOT_KNOWN': unknown})\n",
    "  return datf.assign(**{name: mapped[name] for name in feats})\n",
    "\n",
    "def binnit(datf: DF, \n",
    "           feats: Dict[str, List[int]], \n",
    "           ordinal: bool = True, \n",
    "           todrop: List[str] = to_drop) -> DF: \n",
    "  ''' binning'''\n",
    "  print(\"doing binning\")\n",
    "  assert all([len(x)<26 for x in feats.values()])\n",
    "  \n",
    "  k: List[str] = list(feats.keys())\n",
    "\n",
    "  todrop += k\n",
    "  \n",
    "  j: int = 2 * min([len(x) for x in k]) // 3\n",
    "  \n",
    "  if ordinal: \n",
    "    labels: Dict[str, List[int]] = {name: list(range(1,len(feats[name]))) for name in k}\n",
    "  else: \n",
    "    labels: Dict[str, List[int]] = {name: [ch for ch \n",
    "                                           in \"abcdefghijklmnopqrstuvwxyz\"]\n",
    "                                    [:len(feats[name])-1] \n",
    "              for name in k}\n",
    "  \n",
    "  for name in k: \n",
    "    a: str = name[:j]+'_binned'\n",
    "    feats[a] = feats[name]\n",
    "    \n",
    "  return datf.assign(**{name[:j]+'_binned': pd.cut(datf[name],\n",
    "                                                 bins=feats[name],#break\n",
    "                                                 labels=labels[name]) \n",
    "                        for name in k})\n",
    "\n",
    "\n",
    "def date_to_ord(datf: DF, feats: Union[str, List[str]]) -> DF: \n",
    "  print(\"doing dates\")\n",
    "  if isinstance(feats, str): \n",
    "    feats = [feats]\n",
    "\n",
    "  return datf.assign(**{name: lambda df: (pd\n",
    "                                          .to_datetime(df[name])\n",
    "                                          .apply(lambda x: x.toordinal())) \n",
    "                        for name in feats})\n",
    "\n",
    "def product(datf: DF, feats: Tuple[str,str], todrop: List[str] = to_drop) -> DF: \n",
    "  ''' df['region_district'] = df.apply(lambda row: f'{row.region}_{row.district_code}', axis=1) '''\n",
    "  m = min([len(x) for x in feats])\n",
    "  \n",
    "  todrop += list(feats)\n",
    "  \n",
    "  name = f'{feats[0][:m]}_{feats[1][:m]}'\n",
    "  \n",
    "  return datf.assign(name = lambda df: [f'{le}_{ri}' \n",
    "                                        for le,ri in zip(df[feats[0]], df[feats[1]])])\n",
    "  \n",
    "# df['region_district'] = df.apply(lambda row: f'{row.region}_{row.district_code}', axis=1)\n",
    "  pass\n",
    "\n",
    "\n",
    "def insignificant(datf: DF, \n",
    "                  insgs: List[str], \n",
    "                  thresh: int = 3, \n",
    "                  fillconst: str = \"OTHER\") -> DF: \n",
    "  ''' WARNING: this code doesnt work without impute coming before it! '''\n",
    "  print(\"doing insig drop\")\n",
    "  return datf.assign(**{name: [val \n",
    "                               if (datf[name].str.lower()\n",
    "                                   .value_counts()[val]) > thresh\n",
    "                               else fillconst\n",
    "                               for val \n",
    "                               in datf[name].str.lower()] \n",
    "                        for name in insgs})\n",
    "\n",
    "\n",
    "def droppem(datf: DF, droppin: List[str]) -> DF:\n",
    "  #geog_outliers = [abs(x)+abs(y)>np.exp(-6) \n",
    "  #                 for x,y in zip(datf.longitude, datf.latitude)]\n",
    "  return datf.drop(droppin, axis=1)#[geog_outliers]\n",
    "\n",
    "\n",
    "X = droppem(product(insignificant(date_to_ord(binnit(bool_to_numeric(const_impute(df, \n",
    "                                                                                  null_vals_list), \n",
    "                                                                     boolski), \n",
    "                                                     to_bin), \n",
    "                                              dates), \n",
    "                                  insigs, thresh=2), \n",
    "                    t), \n",
    "            to_drop).replace({\"NOT_KNOWN\": np.nan})\n",
    "\n",
    "print(to_drop)\n",
    "print(X.shape)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#X.to_csv(\"data\", index=False)\n",
    "y = pd.read_csv(csv_github[csv_local[2]])\n",
    "\n",
    "\n",
    "#X.join(y, on='id')\n",
    "X.merge(y.replace({\"functional\": 1, \"non functional\": -1, \"functional needs repair\": 0}), on='id').to_csv('data-large.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59400, 2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
